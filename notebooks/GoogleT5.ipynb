{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from src import data\n",
    "from src.DialoGPT import create_context, chat_with_me\n",
    "from src.GoogleT5 import DatasetT5, train, validate"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "SEED = 23\n",
    "MODEL = 't5-base'\n",
    "\n",
    "MAX_SOURCE_TEXT_LENGTH = 256\n",
    "MAX_TARGET_TEXT_LENGTH = 128\n",
    "\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "TRAIN_EPOCHS = 150\n",
    "\n",
    "OUTPUT_DIR = '../outputs/GoogleT5'\n",
    "\n",
    "CHAR_NAME = 'Iroh'\n",
    "CONTEXT_LENGTH = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and fine-tuning model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model]: Loading t5-base...\n",
      "\n",
      "[Data]: Reading data...\n",
      "\n",
      "FULL Dataset: (337, 2)\n",
      "TRAIN Dataset: (270, 2)\n",
      "TEST Dataset: (67, 2)\n",
      "\n",
      "[Initiating Fine Tuning]...\n",
      "\n",
      "Training loss: 0.9849872589111328\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.6086483597755432\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.5128365159034729\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.45566996932029724\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.40663233399391174\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.3492170572280884\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.30882489681243896\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.26846569776535034\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.24066777527332306\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.2067064344882965\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.18099108338356018\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.1560547947883606\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.14167064428329468\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.1276826113462448\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.11518648266792297\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.10277331620454788\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.09780456125736237\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.08635763078927994\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.08076711744070053\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0748421922326088\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.06759291887283325\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.06456346064805984\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0581158883869648\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.057112500071525574\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.05485646799206734\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.05013151466846466\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.04913178086280823\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.04299456998705864\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.04337752237915993\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.04160979762673378\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.03658507764339447\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.036319978535175323\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.032365720719099045\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.030651750043034554\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.02723412960767746\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.025076361373066902\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.022444145753979683\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.020877549424767494\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.018297014757990837\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.021869532763957977\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.019213169813156128\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.016616536304354668\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.02116296999156475\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0378083772957325\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.024635573849081993\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.018787849694490433\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.014917917549610138\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.01350677851587534\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.009442877024412155\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.010245287790894508\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.009505667723715305\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.00868298951536417\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.006896481849253178\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.007609677966684103\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.006011319346725941\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.005446028430014849\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.007452423684298992\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.008027996867895126\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.005189016927033663\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.005774297751486301\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.007027079351246357\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.006827032659202814\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.006282511632889509\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.005878521595150232\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.005076627247035503\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004618342500180006\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004997666925191879\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004006176721304655\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.003850023029372096\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004715725779533386\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004193694330751896\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0041390773840248585\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004166715312749147\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004852240905165672\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0039359224028885365\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004594104364514351\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004202407319098711\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.003790556453168392\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.003520393744111061\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0042259665206074715\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004758175928145647\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004910427611321211\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.005095956847071648\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0036953038070350885\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.002963423263281584\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0028345566242933273\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.003534273710101843\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0026919664815068245\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0023466935381293297\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0023789051920175552\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.002567565068602562\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0031498149037361145\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0029168303590267897\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.002231189515441656\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0022495375014841557\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0017911321483552456\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0022730103228241205\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0018323962576687336\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.002024843357503414\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0016681195702403784\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.002376033691689372\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.004284832626581192\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.002887129317969084\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0034166446421295404\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0025159595534205437\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.002328802365809679\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0037457519210875034\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.002714343834668398\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0025912094861268997\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.003640473820269108\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.0027885905001312494\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.002074267715215683\n",
      "[Saving Model]...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 74>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     80\u001B[0m model\u001B[38;5;241m.\u001B[39msave_pretrained(path)\n\u001B[0;32m     81\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39msave_pretrained(path)\n\u001B[1;32m---> 83\u001B[0m contexts, predictions, actuals \u001B[38;5;241m=\u001B[39m \u001B[43mvalidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     84\u001B[0m final_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContext\u001B[39m\u001B[38;5;124m\"\u001B[39m: contexts, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerated Text\u001B[39m\u001B[38;5;124m\"\u001B[39m: predictions, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mActual Text\u001B[39m\u001B[38;5;124m\"\u001B[39m: actuals})\n\u001B[0;32m     85\u001B[0m final_df\u001B[38;5;241m.\u001B[39mto_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(OUTPUT_DIR, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredictions.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[1;32mC:\\Artificial Intelligence\\S6\\NLP\\atla-generator\\src\\GoogleT5.py:134\u001B[0m, in \u001B[0;36mvalidate\u001B[1;34m(tokenizer, model, device, loader)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m#mask = data[\"source_mask\"].to(device, dtype=torch.long)\u001B[39;00m\n\u001B[0;32m    126\u001B[0m generated_ids \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m    127\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39mids,\n\u001B[0;32m    128\u001B[0m     max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    131\u001B[0m     repetition_penalty\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2.5\u001B[39m\n\u001B[0;32m    132\u001B[0m     )\n\u001B[1;32m--> 134\u001B[0m context \u001B[38;5;241m=\u001B[39m [tokenizer\u001B[38;5;241m.\u001B[39mdecode(c, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m ids]\n\u001B[0;32m    135\u001B[0m preds \u001B[38;5;241m=\u001B[39m [tokenizer\u001B[38;5;241m.\u001B[39mdecode(g, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m generated_ids]\n\u001B[0;32m    136\u001B[0m target \u001B[38;5;241m=\u001B[39m [tokenizer\u001B[38;5;241m.\u001B[39mdecode(t, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m y]\n",
      "File \u001B[1;32mC:\\Artificial Intelligence\\S6\\NLP\\atla-generator\\src\\GoogleT5.py:134\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m#mask = data[\"source_mask\"].to(device, dtype=torch.long)\u001B[39;00m\n\u001B[0;32m    126\u001B[0m generated_ids \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m    127\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39mids,\n\u001B[0;32m    128\u001B[0m     max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    131\u001B[0m     repetition_penalty\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2.5\u001B[39m\n\u001B[0;32m    132\u001B[0m     )\n\u001B[1;32m--> 134\u001B[0m context \u001B[38;5;241m=\u001B[39m [\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m ids]\n\u001B[0;32m    135\u001B[0m preds \u001B[38;5;241m=\u001B[39m [tokenizer\u001B[38;5;241m.\u001B[39mdecode(g, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m generated_ids]\n\u001B[0;32m    136\u001B[0m target \u001B[38;5;241m=\u001B[39m [tokenizer\u001B[38;5;241m.\u001B[39mdecode(t, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m y]\n",
      "File \u001B[1;32mC:\\Artificial Intelligence\\S6\\NLP\\atla-generator\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3326\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.decode\u001B[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[0;32m   3323\u001B[0m \u001B[38;5;66;03m# Convert inputs to python lists\u001B[39;00m\n\u001B[0;32m   3324\u001B[0m token_ids \u001B[38;5;241m=\u001B[39m to_py_obj(token_ids)\n\u001B[1;32m-> 3326\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decode(\n\u001B[0;32m   3327\u001B[0m     token_ids\u001B[38;5;241m=\u001B[39mtoken_ids,\n\u001B[0;32m   3328\u001B[0m     skip_special_tokens\u001B[38;5;241m=\u001B[39mskip_special_tokens,\n\u001B[0;32m   3329\u001B[0m     clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39mclean_up_tokenization_spaces,\n\u001B[0;32m   3330\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3331\u001B[0m )\n",
      "File \u001B[1;32mC:\\Artificial Intelligence\\S6\\NLP\\atla-generator\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:928\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._decode\u001B[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_decode\u001B[39m(\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    920\u001B[0m     token_ids: List[\u001B[38;5;28mint\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    924\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    925\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m    926\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decode_use_source_tokenizer \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_source_tokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m--> 928\u001B[0m     filtered_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_ids_to_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    930\u001B[0m     \u001B[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001B[39;00m\n\u001B[0;32m    931\u001B[0m     \u001B[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001B[39;00m\n\u001B[0;32m    932\u001B[0m     \u001B[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001B[39;00m\n\u001B[0;32m    933\u001B[0m     sub_texts \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mC:\\Artificial Intelligence\\S6\\NLP\\atla-generator\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:904\u001B[0m, in \u001B[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001B[1;34m(self, ids, skip_special_tokens)\u001B[0m\n\u001B[0;32m    902\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m ids:\n\u001B[0;32m    903\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(index)\n\u001B[1;32m--> 904\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m skip_special_tokens \u001B[38;5;129;01mand\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_special_ids\u001B[49m:\n\u001B[0;32m    905\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    906\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madded_tokens_decoder:\n",
      "File \u001B[1;32mC:\\Artificial Intelligence\\S6\\NLP\\atla-generator\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1254\u001B[0m, in \u001B[0;36mSpecialTokensMixin.all_special_ids\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1250\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1251\u001B[0m \u001B[38;5;124;03m`List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\u001B[39;00m\n\u001B[0;32m   1252\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1253\u001B[0m all_toks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_special_tokens\n\u001B[1;32m-> 1254\u001B[0m all_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_tokens_to_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_toks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1255\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m all_ids\n",
      "File \u001B[1;32mC:\\Artificial Intelligence\\S6\\NLP\\atla-generator\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:578\u001B[0m, in \u001B[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001B[1;34m(self, tokens)\u001B[0m\n\u001B[0;32m    576\u001B[0m ids \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m tokens:\n\u001B[1;32m--> 578\u001B[0m     ids\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_token_to_id_with_added_voc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "File \u001B[1;32mC:\\Artificial Intelligence\\S6\\NLP\\atla-generator\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:587\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001B[1;34m(self, token)\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madded_tokens_encoder:\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madded_tokens_encoder[token]\n\u001B[1;32m--> 587\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_token_to_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Artificial Intelligence\\S6\\NLP\\atla-generator\\venv\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:291\u001B[0m, in \u001B[0;36mT5Tokenizer._convert_token_to_id\u001B[1;34m(self, token)\u001B[0m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;124;03m\"\"\"Converts a token (str) in an id using the vocab.\"\"\"\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m token\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<extra_id_\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 291\u001B[0m     match \u001B[38;5;241m=\u001B[39m \u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<extra_id_(\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43md+)>\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    292\u001B[0m     num \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(match\u001B[38;5;241m.\u001B[39mgroup(\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_size \u001B[38;5;241m-\u001B[39m num \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\re.py:190\u001B[0m, in \u001B[0;36mmatch\u001B[1;34m(pattern, string, flags)\u001B[0m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmatch\u001B[39m(pattern, string, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    188\u001B[0m     \u001B[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning\u001B[39;00m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmatch(string)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\re.py:288\u001B[0m, in \u001B[0;36m_compile\u001B[1;34m(pattern, flags)\u001B[0m\n\u001B[0;32m    285\u001B[0m _cache \u001B[38;5;241m=\u001B[39m {}  \u001B[38;5;66;03m# ordered!\u001B[39;00m\n\u001B[0;32m    287\u001B[0m _MAXCACHE \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m512\u001B[39m\n\u001B[1;32m--> 288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_compile\u001B[39m(pattern, flags):\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;66;03m# internal: compile pattern\u001B[39;00m\n\u001B[0;32m    290\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(flags, RegexFlag):\n\u001B[0;32m    291\u001B[0m         flags \u001B[38;5;241m=\u001B[39m flags\u001B[38;5;241m.\u001B[39mvalue\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(SEED)  # pytorch random seed\n",
    "np.random.seed(SEED)  # numpy random seed\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# logging\n",
    "print(f\"\"\"[Model]: Loading {MODEL}...\\n\"\"\")\n",
    "\n",
    "# tokenizer for encoding the text\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL, model_max_length=MAX_SOURCE_TEXT_LENGTH)\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
    "model = model.to(device)\n",
    "\n",
    "# logging\n",
    "print(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "# Importing the raw dataset\n",
    "dialogue_df = data.read_dataframe()\n",
    "context_df = create_context(dialogue_df, CHAR_NAME, CONTEXT_LENGTH)\n",
    "\n",
    "# Creation of Dataset and Dataloader\n",
    "# Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "train_size = 0.8\n",
    "train_dataset = context_df.sample(frac=train_size, random_state=SEED)\n",
    "val_dataset = context_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(f\"FULL Dataset: {context_df.shape}\")\n",
    "print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "# Creating the Training and Validation dataset for further creation of Dataloader\n",
    "training_set = DatasetT5(\n",
    "    train_dataset,\n",
    "    tokenizer,\n",
    "    MAX_SOURCE_TEXT_LENGTH,\n",
    "    MAX_TARGET_TEXT_LENGTH\n",
    ")\n",
    "val_set = DatasetT5(\n",
    "    val_dataset,\n",
    "    tokenizer,\n",
    "    MAX_SOURCE_TEXT_LENGTH,\n",
    "    MAX_TARGET_TEXT_LENGTH\n",
    ")\n",
    "\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "    \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0,\n",
    "}\n",
    "\n",
    "val_params = {\n",
    "    \"batch_size\": VALID_BATCH_SIZE,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 0,\n",
    "}\n",
    "\n",
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(), lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(f\"[Initiating Fine Tuning]...\\n\")\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    train(tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "    print(f\"[Saving Model]...\\n\")\n",
    "    # Saving the model after training\n",
    "    path = os.path.join(OUTPUT_DIR, \"model_files\", f\"epoch-{epoch}\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "    contexts, predictions, actuals = validate(tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({\"Context\": contexts, \"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "    final_df.to_csv(os.path.join(OUTPUT_DIR, \"predictions.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}