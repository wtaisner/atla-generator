{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from src import data\n",
    "from src.DialoGPT import create_context, chat_with_me\n",
    "from src.GoogleT5 import DatasetT5, train, validate"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "SEED = 23\n",
    "MODEL = 't5-base'\n",
    "\n",
    "MAX_SOURCE_TEXT_LENGTH = 256\n",
    "MAX_TARGET_TEXT_LENGTH = 128\n",
    "\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "\n",
    "LEARNING_RATE = 5e-5\n",
    "TRAIN_EPOCHS = 50\n",
    "\n",
    "OUTPUT_DIR = '../outputs/GoogleT5'\n",
    "\n",
    "CHAR_NAME = 'Iroh'\n",
    "CONTEXT_LENGTH = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and fine-tuning model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model]: Loading t5-base...\n",
      "\n",
      "[Data]: Reading data...\n",
      "\n",
      "FULL Dataset: (337, 2)\n",
      "TRAIN Dataset: (270, 2)\n",
      "TEST Dataset: (67, 2)\n",
      "\n",
      "[Initiating Fine Tuning]...\n",
      "\n",
      "Training loss: 1.1339337825775146\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.7070589661598206\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.6169018149375916\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.552310585975647\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.507283091545105\n",
      "[Saving Model]...\n",
      "\n",
      "Training loss: 0.46700868010520935\n",
      "[Saving Model]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(SEED)  # pytorch random seed\n",
    "np.random.seed(SEED)  # numpy random seed\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# logging\n",
    "print(f\"\"\"[Model]: Loading {MODEL}...\\n\"\"\")\n",
    "\n",
    "# tokenizer for encoding the text\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL, model_max_length=MAX_SOURCE_TEXT_LENGTH)\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
    "model = model.to(device)\n",
    "\n",
    "# logging\n",
    "print(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "# Importing the raw dataset\n",
    "dialogue_df = data.read_dataframe()\n",
    "context_df = create_context(dialogue_df, CHAR_NAME, CONTEXT_LENGTH)\n",
    "\n",
    "# Creation of Dataset and Dataloader\n",
    "# Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "train_size = 0.8\n",
    "train_dataset = context_df.sample(frac=train_size, random_state=SEED)\n",
    "val_dataset = context_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(f\"FULL Dataset: {context_df.shape}\")\n",
    "print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "# Creating the Training and Validation dataset for further creation of Dataloader\n",
    "training_set = DatasetT5(\n",
    "    train_dataset,\n",
    "    tokenizer,\n",
    "    MAX_SOURCE_TEXT_LENGTH,\n",
    "    MAX_TARGET_TEXT_LENGTH\n",
    ")\n",
    "val_set = DatasetT5(\n",
    "    val_dataset,\n",
    "    tokenizer,\n",
    "    MAX_SOURCE_TEXT_LENGTH,\n",
    "    MAX_TARGET_TEXT_LENGTH\n",
    ")\n",
    "\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "    \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0,\n",
    "}\n",
    "\n",
    "val_params = {\n",
    "    \"batch_size\": VALID_BATCH_SIZE,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 0,\n",
    "}\n",
    "\n",
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(), lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(f\"[Initiating Fine Tuning]...\\n\")\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    train(tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "    print(f\"[Saving Model]...\\n\")\n",
    "    # Saving the model after training\n",
    "    path = os.path.join(OUTPUT_DIR, \"model_files\", f\"epoch-{epoch}\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "    contexts, predictions, actuals = validate(tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({\"Context\": contexts, \"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "    final_df.to_csv(os.path.join(OUTPUT_DIR, \"predictions.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def chat_with_me(model, tokenizer, steps: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    chatting with trained model\n",
    "    :param model: trained model, in general it should be an object of type GPT2LMHeadModel\n",
    "    :param tokenizer: tokenizer for given model\n",
    "    :param steps: the length of the talk (number of phrases we wish to write)\n",
    "    \"\"\"\n",
    "\n",
    "    for step in range(steps):\n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "        user_answer = tokenizer.encode(input(\">> User:\"), return_tensors='pt')\n",
    "        #user_ids = user_answer[\"input_ids\"].squeeze().to_dtype(dtype=torch.long)\n",
    "        #user_mask = user_answer[\"attention_mask\"].squeeze().to_dtype(dtype=torch.long)\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=user_answer.to(device),\n",
    "            #attention_mask=user_mask,\n",
    "            max_length=150,\n",
    "            num_beams=10\n",
    "        )\n",
    "\n",
    "        # pretty print last output tokens from bot\n",
    "        #print(\"\\nBot: {}\".format(\n",
    "        #    tokenizer.decode(generated_ids, skip_special_tokens=True)))\n",
    "        print(tokenizer.decode(generated_ids.cpu().detach()[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chat_with_me(model, tokenizer, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}