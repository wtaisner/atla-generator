{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from src import data\n",
    "from src.DialoGPT import create_context, chat_with_me\n",
    "from src.GoogleT5 import DatasetT5, train, validate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SEED = 23\n",
    "MODEL = 't5-base'\n",
    "\n",
    "MAX_SOURCE_TEXT_LENGTH = 256\n",
    "MAX_TARGET_TEXT_LENGTH = 128\n",
    "\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "TRAIN_EPOCHS = 150\n",
    "\n",
    "OUTPUT_DIR = '../outputs/GoogleT5'\n",
    "\n",
    "CHAR_NAME = 'Iroh'\n",
    "CONTEXT_LENGTH = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and fine-tuning model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(SEED)  # pytorch random seed\n",
    "np.random.seed(SEED)  # numpy random seed\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# logging\n",
    "print(f\"\"\"[Model]: Loading {MODEL}...\\n\"\"\")\n",
    "\n",
    "# tokenizer for encoding the text\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL, model_max_length=MAX_SOURCE_TEXT_LENGTH)\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
    "model = model.to(device)\n",
    "\n",
    "# logging\n",
    "print(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "# Importing the raw dataset\n",
    "dialogue_df = data.read_dataframe()\n",
    "context_df = create_context(dialogue_df, CHAR_NAME, CONTEXT_LENGTH)\n",
    "\n",
    "# Creation of Dataset and Dataloader\n",
    "# Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "train_size = 0.8\n",
    "train_dataset = context_df.sample(frac=train_size, random_state=SEED)\n",
    "val_dataset = context_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(f\"FULL Dataset: {context_df.shape}\")\n",
    "print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "# Creating the Training and Validation dataset for further creation of Dataloader\n",
    "training_set = DatasetT5(\n",
    "    train_dataset,\n",
    "    tokenizer,\n",
    "    MAX_SOURCE_TEXT_LENGTH,\n",
    "    MAX_TARGET_TEXT_LENGTH\n",
    ")\n",
    "val_set = DatasetT5(\n",
    "    val_dataset,\n",
    "    tokenizer,\n",
    "    MAX_SOURCE_TEXT_LENGTH,\n",
    "    MAX_TARGET_TEXT_LENGTH\n",
    ")\n",
    "\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "    \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0,\n",
    "}\n",
    "\n",
    "val_params = {\n",
    "    \"batch_size\": VALID_BATCH_SIZE,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 0,\n",
    "}\n",
    "\n",
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(), lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(f\"[Initiating Fine Tuning]...\\n\")\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    train(tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "    print(f\"[Saving Model]...\\n\")\n",
    "    # Saving the model after training\n",
    "    path = os.path.join(OUTPUT_DIR, \"model_files\", f\"epoch-{epoch}\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "    contexts, predictions, actuals = validate(tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({\"Context\": contexts, \"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "    final_df.to_csv(os.path.join(OUTPUT_DIR, \"predictions.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}